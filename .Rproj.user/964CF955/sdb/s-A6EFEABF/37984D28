{
    "contents" : "---\ntitle: \"CourseraMachineLearning\"\nauthor: \"JoshuaWei\"\ndate: \"October 25, 2015\"\noutput: html_document\n---\n\nOverview and Introduction: Utilizing devices with both accelerometers and gyrometers, we can get a rich amount of data to better parse the biomechanics of certain actions and motions. With this data, from new mobile monitoring machines like the fitbit, we can build a profile and predict the activity of an individual based on the profile of the action. With out dataset, there are some challenges needed to be addressed before we can proceed with analysis. Our basic protocol will involve a data scan and overview to understand what we're dealing with, data transformation and variable selection, our model fitting with a random forest (ensemble CART model), and cross validation.\n\nData Overview: We received two datasets, a training and a testing set. I began with a survey of the testing set, revealing both large number of observations and rows. There are 160 variables and 19622 observations. However we notice that a huge proportion of these columns are either incredibly sparse or simply 'NA.'\n\nprint(cars)\n```{R}\nlibrary(caret)\nlibrary(rpart)\nlibrary(AppliedPredictiveModeling)\ntrainset<-read.csv(file=\"trainset.csv\", header=TRUE)\ntestset<-read.csv(file=\"testset.csv\", header=TRUE)\nprint(trainset)\n```\n\nVariable Selection: I decided to omit the majority of variables that have too many NAs or sparse variables. We would otherwise be forced to omit many observations if we decided to focus on those factors. We ended up settling on 41 factors, a combination of numerical and categorical data, that were not sparsely filled for our analysis.\n```{R}\nkeepthis <- c(\"gyros_belt_x\",\"gyros_belt_y\",\"gyros_belt_z\",\"accel_belt_x\",\"accel_belt_y\",\"accel_belt_z\",\"magnet_belt_x\",\"magnet_belt_y\",\"magnet_belt_z\",\"roll_arm\",\"pitch_arm\",\"yaw_arm\",\"total_accel_arm\",\"roll_belt\",\"pitch_belt\",\"yaw_belt\",\"total_accel_belt\",\"classe\",\"total_accel_forearm\",\"user_name\",\"gyros_dumbbell_x\",\"gyros_dumbbell_y\",\"gyros_dumbbell_z\",\"accel_dumbbell_x\",\"accel_dumbbell_y\",\"accel_dumbbell_z\",\"magnet_dumbbell_x\",\"magnet_dumbbell_y\",\"magnet_dumbbell_z\",\"roll_forearm\",\"pitch_forearm\",\"yaw_forearm\",\"gyros_forearm_x\",\"gyros_forearm_y\",\"gyros_forearm_z\",\"accel_forearm_x\",\"accel_forearm_y\",\"accel_forearm_z\",\"magnet_forearm_x\",\"magnet_forearm_y\",\"magnet_forearm_z\")\ntrainsetp <- trainset[,keepthis]\n\n```\n\n\n\nDiscussion on Model Fitting: We attempt to address a classification problem, leading to an idea application for Classification/Regression Tree or a CART. \n\n```{r}\nmodFit <-train(classe~., data=trainsetp, method=\"rpart\")\nprint(modFit$finalModel)\npred <- predict(modFit, testset)#outputs our values.\n\nplot(modFit$finalModel, uniform=TRUE, main=\"Classification Tree\")\ntext(modFit$finalModel, use.n=TRUE, all=TRUE, cex=0.7, position=1)\n```\n\n\n\nDiscussion: We found that rollbelt and pitch forearm provided the most significant high end predictors but the ones that provided hte most accuracy were pitchforearm, rollform, and magnet dumbell.\n\n\n\nValidation of Results: We execute a cross validation for our results and to estimate our out of sample error. We expect out of sample results to have a higher error rate, reflected with our cross validation that will probablyyield results with lower accuracy but with lower bias. We confirmed our predictive accuracy to drop in our cross validated set which mimics error out of sample. Accuracy is 49.3%, and against a benchmark out of 5 outcomes whererandomly would give us 20%, we demonstrate a slightly better positive predictive value.\n```{R}\ninTrain<- createDataPartition(y=trainsetp$classe, p=0.70, list=FALSE)\n\nvalidtrain <- trainsetp[inTrain,]\nvalidtest <- trainsetp[-inTrain,]\n\nvalidmodFit <-train(classe~., data=validtrain, method=\"rpart\")\nprint(modFit$finalModel)\nvalidpred <- predict(validmodFit, validtest) #outputs our values.\n\naccuracy <-cbind(validtest,validpred)\naccuracy$accuracy <- ifelse(accuracy$validpred==accuracy$class,1,0)\n\nAccuracy<-sum(accuracy$accuracy)/nrow(accuracy)\n```\n\n\nHowever, by utilizing an ensemble model like a Random Forest, we can minimize the effects of overfitting and therefore decrease our out of sample error comparatively! We regress against our categorical varriable Classe, as randomforests are traditionally high performing, as per lecture and accurate. However, we have some issues with data computation so need to do PCA first. We had some issues with computational availability, so therefore we had to use a PCA to reduce the number of variables from 41 to something more manageable.\n\n```{r}\nnums <-sapply(trainsetp,is.numeric)\nprePCA.trainsetp<-trainsetp[nums]\npreProc<-preProcess(prePCA.trainsetp,method=\"pca\",pcaComp=2)\ntrainPC<-predict(preProc, trainsetp)\ntestPC<-predict(preProc, testset)\nnewnames <-c(\"user_name\", \"PC1\",       \"PC2\")\ntestPC<-testPC[,newnames]\n\n```\n\n```{r}\nmodFit2 <-train(classe~., data=trainPC, method=\"rf\")\nprint(modFit$finalModel)\npred2 <- predict(modFit, testPC) #outputs our values.\n```",
    "created" : 1445794471755.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1263568657",
    "id" : "37984D28",
    "lastKnownWriteTime" : 1445808884,
    "path" : "C:/Users/577731/Desktop/Coursera/Rmarkdown.Rmd",
    "project_path" : "Rmarkdown.Rmd",
    "properties" : {
        "tempName" : "Untitled2"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}